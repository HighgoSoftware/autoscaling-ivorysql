# Integrated scheduler

This builds on [`scaling_builtin`], which in turn builds on [`kind_setup`]. For more detail, refer
to their respective READMEs (particularly [`kind_setup`]).

[`scaling_builtin`]: ../scaling_builtin
[`kind_setup`]: ../kind_setup

Steps:

Build everything:

```sh
vm_image/start-local-registry.sh # required for everything below. Does nothing on repeat
vm_image/build.sh
build/autoscale-scheduler/build.sh
build/autoscaler-agent/build.sh
```

We also require a local build of NeonVM (for now, as of 2022-11-27); which can be done by cloning
the repository:
```sh
git clone -b sharnoff/dev git@github.com:neondatabase/neonvm
cd virtink # ^^^^^^^^^^^^ NOTE: needs to be the right branch.

# as root:
CONTROLLER_IMG='localhost:5001/neonvm-controller:latest' \
RUNNER_IMG='localhost:5001/neonvm-runner:latest' \
./build_docker.sh
```

Download various dependencies:

```sh
scripts/download-cni.sh
scripts/download-deployments.sh
```

Set up the cluster:

```sh
scripts/cluster-init.sh
```

Run the VM(s):

```sh
kubectl apply -f vm-deploy.yaml
# or:
kubectl apply -f vm-double-deploy.yaml
```

Run pgbench and watch the vCPU allocation grow:

```sh
scripts/run-bench.sh
# or:
VM_NAME=postgres14-disk-1 scripts/run-bench.sh
VM_NAME=postgres14-disk-2 scripts/run-bench.sh
```

## Architecture

Broadly speaking, the components are:

* `autoscaler-agent`:
  * Gets metrics from VM's `node_exporter` (`http://10.255.255.254:9100/metrics`)
  * Uses `NeonVM` patch requests to resize vCPU and memory
  * Requests vCPU/memory increase, notify decrease to the scheduler plugin (configured port `10299`)
* `scheduler`:
  * Provides the `AutoscaleEnforcer` scheduler plugin, using the `kube-scheduler` plugin interface
  * Handles requests / notifications from `autoscaler-agent`s, limiting them to the node's capacity.
      (Currently reserves some resources for system tasks, via the ConfigMap in `deploy/scheduler-deploy.yaml`.)
* `vm_image`:
  * The normal postgres VM + `node_exporter` image that we've used in other places.
  * **NOTE:** The entire setup requires the registry at `localhost:5001` from `start-local-registry.sh`
* `deploy/neonvm.yaml`:
  * Autogenerated NeonVM deployment from <https://github.com/neondatabase/neonvm>, using Kustomize

Some basics on the `autoscaler-agent` \<-\> `scheduler` protocol:

* All messages are `autoscaler-agent` -> `scheduler`, plus response. The agent sends an
    `AgentRequest` and the scheduler plugin replies with a `PluginResponse`
* All `AgentRequest` contain metrics information and a resource request (may be equal to current)
* When the `autoscaler-agent` wants to *decrease* resource allocation, it does that immediately and
    the `AgentRequest` is informative (i.e., "Hey scheduler, just so you know, I've decreased my
    vCPU. You can allocate that elsewhere").
* When the `autoscaler-agent` wants to *increase* vCPU, it submits the `AgentRequest` and the
    `PluginResponse` contains a `Permit` for resources that may be lower than the requested resource
    amount, but not lower than the current resources (i.e., "Hey scheduler, can I increase to `X`
    vCPUs?" ... "You can only go to `Y`", where `current <= Y <= X`).
  * When there aren't enough resources to satisfy all of the request, the amount unavailable is
      added as "pressure"
* If a node's "pressure" is more than the amount currently being migrated, new pods will selected
    for migration, prioritized based on their reported metrics (lower means more likely).
    * "pressure" is defined as resource allocation above the "watermark" (a value that we try to
        keep usage less than or equal to). Watermarks are set in the `scheduler`'s ConfigMap, in
        `deploy/scheduler-deploy.yaml`.
* The definition of a "compute unit" is defined per-node by the `scheduler`, and all `AgentRequest`s
    must have resources as a multiple of the _last_ compute unit they received from the `scheduler`
    * We say the "last" compute unit, because it can - in theory - be changed at runtime
* The `scheduler` treats resource types independently: it may respond with a `Permit` that is not a
    multiple of the compute units, if there aren't enough resources to fully satisfy the
    `autsocaler-agent`'s request.
    * This approach allows the scheduler to be much simpler, while granting us greater resilience
      and flexibility, and ensuring that resource usage is still _eventually_ balanced according to
      the configured compute unit size.

The files implementing the protocol are in `pkg/agent/run.go` and `pkg/plugin/run.go`, plus API
types in `pkg/api/types.go`.

Currently, the scheduler also appropriately handles pod un-scheduling via `Reserve`/`Unreserve`,
with some initial (but non-binding) capacity checks in the `Filter` step. Pod deletion events are
similarly tracked to release reserved resources.
