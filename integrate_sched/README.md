# Integrated scheduler

This builds on [`scaling_builtin`], which in turn builds on [`kind_setup`]. For more detail, refer
to their respective READMEs (particularly [`kind_setup`]).

[`scaling_builtin`]: ../scaling_builtin
[`kind_setup`]: ../kind_setup

Steps:

Build everything:

```sh
vm_image/start-local-registry.sh # required for everything below. Does nothing on repeat
vm_image/build.sh
build/autoscale-scheduler/build.sh
build/autoscaler-agent/build.sh
```

We also require a local build of NeonVM (for now, as of 2022-11-27); which can be done by cloning
the repository:
```sh
git clone -b sharnoff/dev git@github.com:neondatabase/neonvm
cd virtink # ^^^^^^^^^^^^ NOTE: needs to be the right branch.

# as root:
CONTROLLER_IMG='localhost:5001/neonvm-controller:latest' \
RUNNER_IMG='localhost:5001/neonvm-runner:latest' \
./build_docker.sh
```

Download various dependencies:

```sh
scripts/download-cni.sh
scripts/download-deployments.sh
```

Set up the cluster:

```sh
scripts/cluster-init.sh
```

Run the VM(s):

```sh
kubectl apply -f vm-deploy.yaml
# or:
kubectl apply -f vm-double-deploy.yaml
```

Run pgbench and watch the vCPU allocation grow:

```sh
scripts/run-bench.sh
# or:
VM_NAME=postgres14-disk-1 scripts/run-bench.sh
VM_NAME=postgres14-disk-2 scripts/run-bench.sh
```

## Architecture

Broadly speaking, the components are:

* `autoscaler-agent`:
  * Gets metrics from VM's `node_exporter` (`http://10.255.255.254:9100/metrics`)
  * Uses `NeonVM` patch requests to resize vCPU
  * Requests vCPU increase, notify vCPU decrease to the scheduler plugin (configured port `10299`)
* `scheduler`:
  * Provides the `AutoscaleEnforcer` scheduler plugin, using the `kube-scheduler` plugin interface
  * Handles requests / notifications from `autoscaler-agent`s, limiting them to the node's capacity.
      (Currently reserves 20% of node CPU capacity for system tasks.)
* `vm_image`:
  * The normal VM + `node_exporter` image that we've used in other places.
  * **NOTE:** The entire setup requires the registry at `localhost:5001` from `start-local-registry.sh`
* `deploy/neonvm.yaml`:
  * Autogenerated NeonVM deployment from <https://github.com/neondatabase/neonvm>, using Kustomize

Some basics on the `autoscaler-agent` \<-\> `scheduler` protocol:

* All messages are `autoscaler-agent` -> `scheduler`, plus response. The agent sends an
    `AgentRequest` and the scheduler plugin replies with a `PluginResponse`
* All `AgentRequest` contain metrics information and a resource request (may be equal to current)
* When the `autoscaler-agent` wants to *decrease* vCPU, it does that immediately and the
    `AgentRequest` is informative (i.e., "Hey scheduler, just so you know, I've decreased my
    vCPU. You can allocate that elsewhere").
* When the `autoscaler-agent` wants to *increase* vCPU, it submits the `AgentRequest` and the
    `PluginResponse` contains a `ResourcePermit` that may be lower than the requested vCPU amount,
    but not lower than the current vCPU (i.e., "Hey scheduler, can I increase to `X` vCPUs?" ...
    "You can only go to `Y`", where `current <= Y <= X`).
  * When there aren't enough resources to satisfy all of the request, the amount unavailable is
      added as "pressure"
* If a node's "pressure" is more than the amount currently being migrated, new pods will selected
    for migration, prioritized based on their reported metrics (lower means more likely).

The files implementing the protocol are in `pkg/agent/run.go` and `pkg/plugin/run.go`, plus API
types in `pkg/api/types.go`.

Currently, the scheduler also appropriately handles pod un-scheduling via `Reserve`/`Unreserve`,
with some initial (but non-binding) capacity checks in the `Filter` step. Pod deletion events are
similarly tracked to release reserved resources.
